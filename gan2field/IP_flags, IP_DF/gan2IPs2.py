import pandas as pd
import numpy as np
import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
import helper

# Features:2
"""
Num : 8 ----  IP_flags
3    76110 0
2     5694 0.5
1     2090
Name: IP_flags, dtype: int64
Max : 3
Min : 1
Mean : 2.882303859632393

Num : 9 ----  IP_DF
1    76110
0     7784
Name: IP_DF, dtype: int64
Max : 1
Min : 0
Mean : 0.9072162490762152
"""

# Features
NUM_FEATURES = 2
features = ["IP_flags", "IP_DF"]

# Dataset class
class Gan2Data(Dataset):
    def __init__(self,path,transform=None):
        """
            path(string): path to a csv file.
            transform(callable): a function to apply some transform on data.
        """
        self.csv = pd.read_csv(path, index_col=0)
        self.transform = transform
    def __len__(self):
        """
            returns the length of dataset.
        """
        return len(self.csv)
    def __getitem__(self,idx):
        """
            idx(int): index in the pandas.dataframe

            returns a datapoint after applying transformation(if available).
        """
        data = self.csv.iloc[idx].astype("float64")
        data = torch.Tensor(data.values)
        if self.transform:
            data = self.transform(data)
        return data


# Generator Class
class Generator(nn.Module):
    def __init__(self, gen_parameters):
        """
            gen_parameters(list): list object with following contraints
                             gen_parameters[0] will be the input dimension(latent)
                             gen_parameters[-1] will be the output dimension
        """
        super(Generator, self).__init__()
        self.hidden_dim = gen_parameters[1]
        self.z_dim = gen_parameters[0]
        self.gen = nn.Sequential(
            nn.Linear(self.z_dim, self.hidden_dim),
            nn.Linear(self.hidden_dim, self.hidden_dim*2),
            nn.Tanh(),
            nn.Linear(self.hidden_dim*2, self.hidden_dim*4),
            nn.Tanh(),
            nn.Linear(self.hidden_dim*4, self.hidden_dim*2),
            nn.Tanh(),
            nn.Linear(self.hidden_dim*2, gen_parameters[-1])
        )
    def forward(self, noise):
        """
            noise(tensor) : input to NN.

            returns output after running noise through self.gen
        """
        return self.gen(noise)


# Discriminator class
class Discriminator(nn.Module):
    def __init__(self,disc_parameters):
        """
            disc_parameters(list): list object with following contraints
                disc_parameters[0] will be the input dimension(latent)
                disc_parameters[-1] will be the output dimension
        """
        hidden_dim = disc_parameters[1]
        super(Discriminator, self).__init__()
        self.disc = nn.Sequential(
            nn.Linear(disc_parameters[0], hidden_dim),
            nn.Linear(hidden_dim, hidden_dim*2),
            nn.Tanh(),
            nn.Linear(hidden_dim*2, hidden_dim*2),
            nn.Tanh(),
            nn.Linear(hidden_dim*2, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, disc_parameters[-1]),
            nn.LeakyReLU(0.2, inplace=True)
        )
    def forward(self,dinput):
        """
            dinput(tensor) : input to disc NN.

            returns output after running noise through self.disc
        """
        return self.disc(dinput)


# Modified Generator class
class NewGenerator():
    def __init__(self, gen_structure, gen_target, gen_opt, gen_hyper):
        """
            gen_structure(list) : a valid object alike gen_parameters in Generator() class
            gen_target(list): a 2 member object of form [direction, distance].
            gen_opt(torch.optim() object) : Optimizer for the generator.
            gen_hyper(dict) : map with hyper parameters to values.

                gen(Generator() object)
                gen_opt :Generator's optimizer.
                index(int): the feature on which generator is shifted. see shift_gen_samples
                direction(int) : the direction in feature[index] to shift. see shift_gen_samples
                mean_losses(list) : Stores the loss values.

        """
        self.gen = Generator(gen_structure)
        self.gen_opt = gen_opt(self.gen.parameters(), lr=gen_hyper["lr"])
        self.direction = gen_target[0]
        self.distance = gen_target[1]
        self.mean_losses = []

    def shift_gen_samples(self, fakes):
        """
            fakes(tensor) : fakes generated by self.gen object.

            returns fakes after shifting them by self.direction on self.index dimension.
        """
        val = torch.tensor([self.distance],dtype=torch.float32)
        index = torch.tensor(self.direction)
        fakes.index_add_(1, index, val)
        #fakes[:, self.direction[0]] += self.distance[0]


def get_noise(sample_size, z_dim):
    """
        sample_size(int): dimension to return noise
        z_dim(int) : dimension to return noise

        returns a random tensor of shape (sample_size, z_dim).
    """
    return torch.randn(sample_size, z_dim)


def normalizer(data):
    """
        This is the transformation function for Gan2Data class.
        data(tensor) : data to be transformed.

        returns data after transformation.
    """
    if data[0] == 3.0:
        data[0] = 0.0
    elif data[0] == 2.0:
        data[0] = 0.5
    return data


def calculate_reals(batch):
    """
    returns ground truth labels for batch.
    """
    outputs = torch.empty((0))
    for each in batch:
        if each[0] != 0.0:
            tmp = torch.tensor([[1.0]])
        else:
            tmp = torch.tensor([[1.0]])
        outputs = torch.cat((outputs, tmp))
    return outputs


def printlosses():
    """
        prints the losses of generators and discriminator.
    """
    print("Epoch {} : ".format(epoch))
    print("------------------")
    for i,each in enumerate(generators):
        mean_gen_loss = sum(each.mean_losses[-display_step:]) / display_step
        change = ""
        for k in range(len(each.direction)):
            change+=features[each.direction[k]]
            change+="{} ".format(each.distance[k])
        out = "gen{} {} loss: {}".format(i, change, mean_gen_loss)
        print(out)
        i+=1
    mean_disc_loss = sum(mean_disc_losses[-display_step:]) / display_step
    out = "\ndisc loss = {}".format(mean_disc_loss)
    print(out)
    print("Samples : ")
    print("----------")

    for i,each in enumerate(generators):
        noise = get_noise(1, z_dim)
        fake = each.gen(noise)
        each.shift_gen_samples(fake)
        print("gen {} : {}".format(i, fake))
        print("Disc out : ", end="")
        print(sig(disc(fake)).item())


epochs = 20
lr = 0.01
batch_size=128
criterion = nn.BCEWithLogitsLoss()
z_dim = 4 # Latent dimension

# Dataset initialization
dataset = Gan2Data("./IPs.csv", normalizer)
dataset = DataLoader(dataset, shuffle = True, batch_size=batch_size, drop_last=True)

generator_structure = [z_dim, 3, NUM_FEATURES] # explained in NewGenerator() class.
generator_optimizer = torch.optim.Adam # explained in NewGenerator() class.
generator_hyper_params = {"lr":lr} # explained in NewGenerator() class.


# Loop creates generators and initailizes their index and direction.
generators = []
dist = [1.2,-1.2]
for i in range(NUM_FEATURES*2):
    gen_target = [[int(i/2)]]
    gen_target.append([dist[i%2]])
    gen = NewGenerator(generator_structure, gen_target,generator_optimizer, 
                        generator_hyper_params)
    generators.append(gen)


dist = [0.84,-0.84]
for i in range(NUM_FEATURES*2):
    gen_target = [[0, 1]]
    distance = []
    distance.append(dist[i%2])
    distance.append(dist[int(i/2)])
    gen_target.append(distance)
    gen = NewGenerator(generator_structure, gen_target,generator_optimizer, 
                        generator_hyper_params)
    generators.append(gen)


# Number of generators to be constructed.
num_generators = len(generators)

# Amount of fakes to be generated by each generator during Discriminator Training.
gen_batch_size = int(batch_size/num_generators)


# Initializing discriminator.
disc = Discriminator([2, 4, 1])
disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)

disc.load_state_dict(torch.load("./versions2/disc/1_disc_model.pt"))
disc_opt.load_state_dict(torch.load("./versions2/disc/1_disc_opt.pt"))

mean_disc_losses = []

"""
disc.load_state_dict(torch.load("./versions/04_disc_model.pt"))
disc_opt.load_state_dict(torch.load("./versions/04_disc_opt.pt"))
gen.load_state_dict(torch.load("./versions/041_gen_model.pt"))
gen_opt.load_state_dict(torch.load("./versions/041_gen_opt.pt"))

print(disc_opt.state_dict()["param_groups"])
"""
print(disc)
p = int(input("Num:"))
print(disc.disc[p].weight)

sig = nn.Sigmoid()
helper.view2d(disc, "IP_flags", "IP_DF", -1,4,0.1)

cur_step = 1
display_step = 500
for epoch in range(epochs):
    for cur_batch in dataset:

        # Discriminator Training
        disc.zero_grad()
        fakes = []
        for each in generators:
            noise = get_noise(gen_batch_size*2, z_dim)
            fakes.append(each.gen(noise))
        fakes = torch.cat(tuple(fakes))
        fake_predictions = disc(fakes.detach())
        real_predictions = disc(cur_batch)
        fake_disc_loss = criterion(fake_predictions, torch.zeros_like(fake_predictions))
        real_disc_loss = criterion(real_predictions, calculate_reals(cur_batch))
        disc_loss = (fake_disc_loss*2 + real_disc_loss) / 2
        disc_loss.backward(retain_graph=True)
        disc_opt.step()

        mean_disc_losses += [disc_loss.item()]
        

        # Generators training
        for each in generators:
            each.gen.zero_grad()
            noise = get_noise(batch_size, z_dim)
            fakes = each.gen(noise)
            each.shift_gen_samples(fakes)
            fake_predictions = disc(fakes)
            gen_loss = criterion(fake_predictions, torch.ones_like(fake_predictions))
            gen_loss.backward()
            each.gen_opt.step()

            each.mean_losses += [gen_loss.item()]

        
        if cur_step%display_step==0:
            printlosses()
        if cur_step%100 == 0:
            print(cur_step)
        if cur_step%2000==0:
            helper.view2d(disc, "IP_flags", "IP_DF", -1,4,0.1)
            save = input("Save(Y/N): ")
            if save.lower() == "y":
                helper.ip2save(generators, [disc, disc_opt], features, "./versions2", 1)
        cur_step += 1
helper.view2d(disc, "IP_flags", "IP_DF", -1,4,0.1)
save = input("Save(Y/N): ")
if save.lower() == "y":
    helper.ip2save(generators, [disc, disc_opt], features, "./versions2", 0)
